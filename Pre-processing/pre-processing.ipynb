{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing steps\n",
    "## Konstantina Andronikou "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all relevant packages\n",
    "import ijson\n",
    "import codecs\n",
    "import collections\n",
    "import string \n",
    "import nltk\n",
    "import pycld2 as cld2\n",
    "import cleantext\n",
    "from cleantext import clean\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data\n",
    "filepath = 'file_path'\n",
    "with open(filepath, \"r\") as infile:\n",
    "    objects = ijson.items(infile, '_source.text', multiple_values=True)\n",
    "    column = list(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using a language detection library to store only the English conversations \n",
    "english_conversations = []\n",
    "x=0\n",
    "for convo in column:\n",
    "    try:\n",
    "        _, _, _, detected_language = cld2.detect(convo,  returnVectors=True)\n",
    "        if detected_language[0][3] == 'en' and len(detected_language) == 1: #conditions: if language code is 'en' and if only one is language detected\n",
    "            english_conversations.append(convo)\n",
    "    except:\n",
    "        x+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using an encoding-decoding method to remove non-readable characters \n",
    "decoding = []\n",
    "for i in english_conversations:\n",
    "    string_encode = i.encode(\"ascii\", \"ignore\") # For this project 'ascii' was chosen but any other encoder can be used \n",
    "    i = string_encode.decode()\n",
    "    decoding.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual cleaning of the data \n",
    "clean_convos = []\n",
    "for i in decoding:\n",
    "     without_char = i.replace('‚Äô',\"'\") #this character was frequently seen in the data and therefore it was replaced by the original form of it\n",
    "     clean_convos.append(without_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Generated Responses \n",
    "### The following cell in responsible for removing automatic generated responses from the company - This can be adapted depending on the company "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "automatic_responses = [\"Insert the automatic generated responses here\"]\n",
    "filtered_text = []\n",
    "for i in clean_convos: \n",
    "    if any(x in i for x in automatic_responses):\n",
    "        continue\n",
    "    else:\n",
    "        filtered_text.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the data in terms of emojis \n",
    "without_emojis = []\n",
    "for i in filtered_text:\n",
    "    clean_conversations = clean(i, no_emoji=True)\n",
    "    without_emojis.append(clean_conversations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Check - Optional "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check the most frequent words in the data \n",
    "for i in without_emojis:\n",
    "    words = i.split()\n",
    "resulting_count = collections.Counter(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The output of the previous cell is the most frequent words within the data. Based on human judgment the words that are not relevant to the project are manually stored in an external list for exclusion purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This list exclude all the frequent words that cannot be a topic \n",
    "extra_word_list = []\n",
    "with open ('data/External_List.txt', 'r', encoding= 'utf-8') as f: #the input for this function in the manually created list \n",
    "    frequent_words = f.readlines()\n",
    "    for i in frequent_words:\n",
    "        if '\\n' in i:\n",
    "           lines = i.replace('\\n', '')\n",
    "           extra_word_list.append(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anonymization - Optional \n",
    "### In the case that the data used is not anonymatized and contains sentsitive information such as Names and Surnames, the following cell can be used. This file contains a list with the most frequent names and surnames around the world. \n",
    "### This database was retrived from https://github.com/smashew/NameDatabases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function is loading an external file that contains frequently used names \n",
    "all_names = []\n",
    "with open ('data/Names.txt', 'r', encoding= 'utf-8') as f:\n",
    "    names = f.readlines()\n",
    "    for l in names:\n",
    "        if '\\n' in l:\n",
    "           new_lines = l.replace('\\n', '')\n",
    "           lower_names = new_lines.lower()\n",
    "           all_names.append(lower_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading stop-words \n",
    "stop_words = stopwords.words('english')\n",
    "#stop_words += extra_word_list # optional - if the external list is created\n",
    "#stop_words += all_names # optional - if the previous cell is implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying text mining tools\n",
    "# This step might take a while to run depending on the size of the data\n",
    "final_convos = []\n",
    "lemmatizer = WordNetLemmatizer() # Lemmatization \n",
    "for i, convo in enumerate(without_emojis):\n",
    "    tokenize_words = word_tokenize(convo) #Tokenization \n",
    "    tokenize_words = nltk.pos_tag(tokenize_words) #POS tags \n",
    "    filtered_sample_text=[]\n",
    "    for w, t in tokenize_words:\n",
    "        if 'V' in t:\n",
    "            filtered_sample_text.append(lemmatizer.lemmatize(w, 'v')) #Lemmatization based on verbs\n",
    "        else:\n",
    "            filtered_sample_text.append(lemmatizer.lemmatize(w))\n",
    "    filtered_sample_text = [w for w in filtered_sample_text if w not in stop_words and w.isalpha()] #removing any character that is not in the alphabet\n",
    "    filtered_sample_text = [w for w in filtered_sample_text if not 'pii_' in w] # manual removal of the instance pii_\n",
    "    filtered_sample_text = [w for w in filtered_sample_text if not w in string.punctuation] #removing punctuation \n",
    "\n",
    "    final_convos.append(' '.join(filtered_sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the normalised text into a file \n",
    "import csv\n",
    "with open('data/Input_for_topic_model.tsv', 'w', newline='') as f:\n",
    "    outfile = csv.writer(f)\n",
    "    for convo in final_convos:\n",
    "        outfile.writerow([convo])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Notebook"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
