{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling Technique - BERTopic\n",
    "## Konstantina Andronikou "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Notebook is an adaptation of the tutorial generated from Maarten Grootendorst :\n",
    "https://github.com/MaartenGr/BERTopic/blob/master/bertopic/_bertopic.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import hdbscan\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import gensim.corpora as corpora\n",
    "from typing import List\n",
    "from gensim.matutils import cossim\n",
    "from sklearn.metrics.pairwise import cosine_similarity,cosine_distances\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the pre-processed data generated from pre_processing.ipynb\n",
    "all_text = []\n",
    "with open ('data/Input_for_topic_model.tsv', 'r', encoding = 'utf-8') as f:\n",
    "    text = f.readlines()\n",
    "    for l in text:\n",
    "        if '\\n' in l:\n",
    "           new_lines = l.replace('\\n', '')\n",
    "           all_text.append(new_lines)\n",
    "data = all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading an embedding model - any embedding model can be used in this step\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "embeddings = model.encode(data, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and implementing BERTopic with the following parameters: \n",
    "    num_topics = Number of topics to extract\n",
    "\n",
    "### For additional parameters, please look at: https://maartengr.github.io/BERTopic/api/bertopic.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model \n",
    "Bert_model = BERTopic(nr_topics=10)\n",
    "topics, probabilities = Bert_model.fit_transform(data, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimensionality reduction algorithm\n",
    "umap_embeddings = umap.UMAP(n_neighbors=15, \n",
    "                            n_components=5, \n",
    "                            metric='cosine').fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering the documents\n",
    "import hdbscan\n",
    "cluster = hdbscan.HDBSCAN(min_cluster_size=15,\n",
    "                          metric='euclidean',                      \n",
    "                          cluster_selection_method='eom').fit(umap_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the resulting clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Prepare data\n",
    "umap_data = umap.UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "result = pd.DataFrame(umap_data, columns=['x', 'y'])\n",
    "result['labels'] = cluster.labels_\n",
    "\n",
    "# Visualize clusters\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "outliers = result.loc[result.labels == -1, :]\n",
    "clustered = result.loc[result.labels != -1, :]\n",
    "plt.scatter(outliers.x, outliers.y, color='#BDBDBD', s=0.05)\n",
    "plt.scatter(clustered.x, clustered.y, c=clustered.labels, s=0.05, cmap='hsv_r')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df = pd.DataFrame(data, columns=[\"Doc\"])\n",
    "docs_df['Topic'] = cluster.labels_\n",
    "docs_df['Doc_ID'] = range(len(docs_df))\n",
    "docs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'Doc': ' '.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class-based tf-idf \n",
    "def c_tf_idf(documents, m, ngram_range=(1, 1)):\n",
    "    count = CountVectorizer(ngram_range=ngram_range, stop_words=\"english\").fit(documents)\n",
    "    t = count.transform(documents).toarray()\n",
    "    w = t.sum(axis=1)\n",
    "    tf = np.divide(t.T, w)\n",
    "    sum_t = t.sum(axis=0)\n",
    "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
    "    tf_idf = np.multiply(tf, idf)\n",
    "\n",
    "    return tf_idf, count\n",
    "  \n",
    "tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m=len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20):\n",
    "    words = count.get_feature_names()\n",
    "    labels = list(docs_per_topic.Topic)\n",
    "    tf_idf_transposed = tf_idf.T\n",
    "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
    "    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
    "    return top_n_words\n",
    "\n",
    "def extract_topic_sizes(df):\n",
    "    topic_sizes = (df.groupby(['Topic'])\n",
    "                     .Doc\n",
    "                     .count()\n",
    "                     .reset_index()\n",
    "                     .rename({\"Topic\": \"Topic\", \"Doc\": \"Size\"}, axis='columns')\n",
    "                     .sort_values(\"Size\", ascending=False))\n",
    "    return topic_sizes\n",
    "\n",
    "top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)\n",
    "topic_sizes = extract_topic_sizes(docs_df); topic_sizes.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "def visualize_10_barchart(topic_model,\n",
    "                       topics: List[int] = None,\n",
    "                       top_n_topics: int = 10, \n",
    "                       n_words: int = 10,\n",
    "                       width: int = 250,\n",
    "                       height: int = 250) -> go.Figure:\n",
    "    \"\"\" Visualize a barchart of selected topics\n",
    "    Arguments:\n",
    "        topic_model: A fitted BERTopic instance.\n",
    "        topics: A selection of topics to visualize.\n",
    "        top_n_topics: Only select the top n most frequent topics.\n",
    "        n_words: Number of words to show in a topic\n",
    "        width: The width of each figure.\n",
    "        height: The height of each figure.\n",
    "    Returns:\n",
    "        fig: A plotly figure\n",
    "    Usage:\n",
    "    To visualize the barchart of selected topics\n",
    "    simply run:\n",
    "    ```python\n",
    "    topic_model.visualize_barchart()\n",
    "    ```\n",
    "    Or if you want to save the resulting figure:\n",
    "    ```python\n",
    "    fig = topic_model.visualize_barchart()\n",
    "    fig.write_html(\"path/to/file.html\")\n",
    "    ```\n",
    "    <iframe src=\"../../getting_started/visualization/bar_chart.html\"\n",
    "    style=\"width:1100px; height: 660px; border: 0px;\"\"></iframe>\n",
    "    \"\"\"\n",
    "    colors = itertools.cycle([\"#D55E00\", \"#0072B2\", \"#CC79A7\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\"])\n",
    "\n",
    "    # Select topics based on top_n and topics args\n",
    "    freq_df = topic_model.get_topic_freq()\n",
    "    freq_df = freq_df.loc[freq_df.Topic != -1, :]\n",
    "    if topics is not None:\n",
    "        topics = list(topics)\n",
    "    elif top_n_topics is not None:\n",
    "        topics = sorted(freq_df.Topic.to_list()[:top_n_topics])\n",
    "    else:\n",
    "        topics = sorted(freq_df.Topic.to_list()[0:6])\n",
    "\n",
    "    # Initialize figure\n",
    "    subplot_titles = [f\"Topic {topic}\" for topic in topics]\n",
    "    columns = 4\n",
    "    rows = int(np.ceil(len(topics) / columns))\n",
    "    fig = make_subplots(rows=rows,\n",
    "                        cols=columns,\n",
    "                        shared_xaxes=False,\n",
    "                        horizontal_spacing=.1,\n",
    "                        vertical_spacing=.4 / rows if rows > 1 else 0,\n",
    "                        subplot_titles=subplot_titles)\n",
    "\n",
    "    # Add barchart for each topic\n",
    "    row = 1\n",
    "    column = 1\n",
    "    for topic in topics:\n",
    "        words = [word + \"  \" for word, _ in topic_model.get_topic(topic)][:n_words][::-1]\n",
    "        scores = [score for _, score in topic_model.get_topic(topic)][:n_words][::-1]\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=scores,\n",
    "                   y=words,\n",
    "                   orientation='h',\n",
    "                   marker_color=next(colors)),\n",
    "            row=row, col=column)\n",
    "\n",
    "        if column == columns:\n",
    "            column = 1\n",
    "            row += 1\n",
    "        else:\n",
    "            column += 1\n",
    "\n",
    "    # Stylize graph\n",
    "    fig.update_layout(\n",
    "        template=\"plotly_white\",\n",
    "        showlegend=False,\n",
    "        title={\n",
    "            'text': \"<b>Topic Word Scores\",\n",
    "            'x': .5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top',\n",
    "            'font': dict(\n",
    "                size=22,\n",
    "                color=\"Black\")\n",
    "        },\n",
    "        width=width*4,\n",
    "        height=height*rows if rows > 1 else height * 1.3,\n",
    "        hoverlabel=dict(\n",
    "            bgcolor=\"white\",\n",
    "            font_size=16,\n",
    "            font_family=\"Rockwell\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(showgrid=True)\n",
    "    fig.update_yaxes(showgrid=True)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_10_barchart(Bert_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bert_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "# topics = .show_topics(formatted=False)\n",
    "fig, axes = plt.subplots(5, 2, figsize=(10,10), sharex=True, sharey=True) #setting the number of topics \n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    # topic_words = dict(topics[i][1])\n",
    "    text = {word: value for word, value in Bert_model.get_topic(i)}\n",
    "    wc = WordCloud(background_color='white',\n",
    "                  width=3000,\n",
    "                  height=1900,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "    wc.generate_from_frequencies(text)\n",
    "    plt.gca().imshow(wc)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation \n",
    "### The model was evaluated in terms of coherence score (c_v and u_mass)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = pd.DataFrame({\"Document\": data,\n",
    "                          \"ID\": range(len(data)),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = Bert_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = Bert_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in Bert_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence = coherence_model.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = pd.DataFrame({\"Document\": data,\n",
    "                          \"ID\": range(len(data)),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = Bert_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = Bert_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in Bert_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='u_mass')\n",
    "coherence = coherence_model.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coherence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following cell presents pairwise cosine similarity. An overall score and word-level cosine similarity for all possible word combinations is presented in error_analysis.ipynb. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = Bert_model.get_topic(0) #Topic 1\n",
    "doc2 = Bert_model.get_topic(1) #Topic 2\n",
    "print(cossim(doc1, doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Notebook"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
