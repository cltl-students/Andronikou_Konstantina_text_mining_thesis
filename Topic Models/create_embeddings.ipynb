{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an embeddings model \n",
    "## Konstantina Andronikou "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Notebook is an adaptation of the tutorial generated from Piek Vossen:\n",
    "https://github.com/cltl/ma-hlt-labs/blob/master/lab2.word_meaning/Lab2.3.create_embeddings_in_some_language.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Loading the input file which we want to embedd\n",
    "## For this project the pre-processed file was used \n",
    "text= []\n",
    "with open ('file_path', 'r', encoding= 'utf-8') as f:\n",
    "    convo = f.readlines()\n",
    "    for l in convo:\n",
    "        if '\\n' in l:\n",
    "           new_lines = l.replace('\\n', '')\n",
    "           text.append(new_lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional \n",
    "## If the input data is not pre-processed, the following steps can be implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to remove punctuation from word tokens\n",
    "def remove_punct(tokens):\n",
    "    punct = string.punctuation\n",
    "    tokens_clean = [] # empty list to store the clean tokens\n",
    "\n",
    "    for t in tokens:\n",
    "        if t not in punct:\n",
    "            tokens_clean.append(t)\n",
    "            \n",
    "    return tokens_clean\n",
    "\n",
    "def preprocess_sentences(file):\n",
    "    clean_sentences = []\n",
    "    \n",
    "    with open(file, \"r\") as i:\n",
    "            for sentence in i:\n",
    "                # Lowercase \n",
    "                tokens = word_tokenize(sentence.lower())\n",
    "                tokens_clean = remove_punct(tokens[1:]) # we skip the first token which is the identifier.\n",
    "                clean_sentences.append(tokens_clean)\n",
    "    return clean_sentences\n",
    "\n",
    "def preprocess_rawtext(file):\n",
    "    clean_sentences = []\n",
    "\n",
    "    with open(file) as infile:\n",
    "        text = infile.read()\n",
    "        \n",
    "    sentences = sent_tokenize(text.strip())\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence.lower())\n",
    "        tokens_clean = remove_punct(tokens)\n",
    "        clean_sentences.append(tokens_clean)\n",
    "    return clean_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The input that we would like to pre-process\n",
    "path_to_the_corpus_file='file_path'\n",
    "\n",
    "clean_corpus = preprocess_sentences(path_to_the_corpus_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following step uses Word2Vec to create the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_corpus = Word2Vec(clean_corpus, window =4, min_count =100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_corpus.wv.save_word2vec_format('data/embedding_model.txt') #cretion of the model\n",
    "embedded_corpus.wv.save_word2vec_format('data/embedding_model.bin', binary=True) #saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a stored model:\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "embedded_model = KeyedVectors.load_word2vec_format('data/embedding_model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Vector size =', embedded_model.vector_size)\n",
    "print('Vocabulary size =', len(embedded_model.key_to_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Notebook"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
